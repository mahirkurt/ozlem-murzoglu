"""
Otomatik PDF Ä°zleme ve Ã–ÄŸrenme Sistemi
Yeni PDF'ler eklendiÄŸinde otomatik olarak Pinecone'a yÃ¼kler
"""
import os
import time
import hashlib
import json
from pathlib import Path
from datetime import datetime
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
import logging

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pdf_ingestion.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

class PDFIngestionHandler(FileSystemEventHandler):
    """PDF dosyalarÄ±nÄ± izler ve yeni/deÄŸiÅŸen dosyalarÄ± otomatik olarak iÅŸler"""
    
    def __init__(self):
        self.processed_files = self.load_processed_files()
        self.index_name = "saglik-petegim-rag"
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=os.getenv("GEMINI_API_KEY")
        )
        self.pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        
        # Index kontrolÃ¼
        if self.index_name not in self.pc.list_indexes().names():
            logger.error(f"Pinecone index '{self.index_name}' bulunamadÄ±!")
            raise Exception(f"Index {self.index_name} mevcut deÄŸil")
        
        logger.info("PDF Ingestion Handler baÅŸlatÄ±ldÄ±")
    
    def load_processed_files(self):
        """Daha Ã¶nce iÅŸlenmiÅŸ dosyalarÄ±n hash'lerini yÃ¼kle"""
        processed_file = Path("processed_pdfs.json")
        if processed_file.exists():
            with open(processed_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def save_processed_files(self):
        """Ä°ÅŸlenmiÅŸ dosyalarÄ±n hash'lerini kaydet"""
        with open("processed_pdfs.json", 'w', encoding='utf-8') as f:
            json.dump(self.processed_files, f, indent=2)
    
    def get_file_hash(self, filepath):
        """DosyanÄ±n MD5 hash'ini hesapla"""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def process_pdf(self, pdf_path):
        """Tek bir PDF dosyasÄ±nÄ± iÅŸle ve Pinecone'a yÃ¼kle"""
        try:
            pdf_path = Path(pdf_path)
            
            # Hash kontrolÃ¼
            file_hash = self.get_file_hash(pdf_path)
            file_key = str(pdf_path)
            
            if file_key in self.processed_files:
                if self.processed_files[file_key] == file_hash:
                    logger.info(f"Dosya zaten iÅŸlenmiÅŸ, atlanÄ±yor: {pdf_path.name}")
                    return False
                else:
                    logger.info(f"Dosya gÃ¼ncellenmiÅŸ, yeniden iÅŸleniyor: {pdf_path.name}")
            else:
                logger.info(f"Yeni dosya bulundu: {pdf_path.name}")
            
            # PDF'i yÃ¼kle
            logger.info(f"PDF yÃ¼kleniyor: {pdf_path.name}")
            loader = PyPDFLoader(str(pdf_path))
            documents = loader.load()
            
            # Metadata ekle
            category = pdf_path.parent.name
            for doc in documents:
                doc.metadata['source_file'] = pdf_path.name
                doc.metadata['category'] = category
                doc.metadata['ingestion_date'] = datetime.now().isoformat()
                doc.metadata['file_hash'] = file_hash
            
            # Chunk'lara bÃ¶l
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=150,
                length_function=len
            )
            chunks = text_splitter.split_documents(documents)
            logger.info(f"{len(chunks)} chunk oluÅŸturuldu")
            
            # Eski vektÃ¶rleri temizle (eÄŸer dosya gÃ¼ncellenmiÅŸse)
            if file_key in self.processed_files:
                logger.info(f"Eski vektÃ¶rler temizleniyor: {pdf_path.name}")
                # Not: Pinecone'da namespace veya metadata filter kullanarak
                # eski vektÃ¶rleri silebilirsiniz
            
            # Pinecone'a yÃ¼kle
            logger.info(f"Pinecone'a yÃ¼kleniyor: {pdf_path.name}")
            PineconeVectorStore.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                index_name=self.index_name
            )
            
            # Ä°ÅŸlenen dosyayÄ± kaydet
            self.processed_files[file_key] = file_hash
            self.save_processed_files()
            
            logger.info(f"âœ… BaÅŸarÄ±yla iÅŸlendi: {pdf_path.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Hata ({pdf_path.name}): {e}")
            return False
    
    def on_created(self, event):
        """Yeni dosya eklendiÄŸinde"""
        if not event.is_directory and event.src_path.endswith('.pdf'):
            logger.info(f"Yeni PDF tespit edildi: {event.src_path}")
            time.sleep(2)  # DosyanÄ±n tam yÃ¼klenmesini bekle
            self.process_pdf(event.src_path)
    
    def on_modified(self, event):
        """Dosya deÄŸiÅŸtirildiÄŸinde"""
        if not event.is_directory and event.src_path.endswith('.pdf'):
            logger.info(f"PDF gÃ¼ncellendi: {event.src_path}")
            time.sleep(2)  # DosyanÄ±n tam yazÄ±lmasÄ±nÄ± bekle
            self.process_pdf(event.src_path)
    
    def scan_existing_pdfs(self, directory):
        """Mevcut tÃ¼m PDF'leri tara ve iÅŸlenmemiÅŸ olanlarÄ± yÃ¼kle"""
        pdf_dir = Path(directory)
        pdf_files = list(pdf_dir.glob("**/*.pdf"))
        
        logger.info(f"Toplam {len(pdf_files)} PDF dosyasÄ± bulundu")
        
        new_files = 0
        for pdf_file in pdf_files:
            if self.process_pdf(pdf_file):
                new_files += 1
                time.sleep(1)  # Rate limiting iÃ§in
        
        logger.info(f"Tarama tamamlandÄ±: {new_files} yeni/gÃ¼ncellenmiÅŸ dosya iÅŸlendi")
        return new_files

def main():
    """Ana fonksiyon - PDF klasÃ¶rÃ¼nÃ¼ izle ve otomatik Ã¶ÄŸren"""
    
    # Ä°zlenecek klasÃ¶r
    watch_directory = Path("D:/GitHub Repos/Saglik-Petegim/assets/Chatbot")
    
    if not watch_directory.exists():
        logger.error(f"KlasÃ¶r bulunamadÄ±: {watch_directory}")
        return
    
    logger.info("="*60)
    logger.info("Otomatik PDF Ã–ÄŸrenme Sistemi BaÅŸlatÄ±lÄ±yor")
    logger.info("="*60)
    
    # Handler oluÅŸtur
    event_handler = PDFIngestionHandler()
    
    # Ä°lk tarama - mevcut dosyalarÄ± kontrol et
    logger.info("Mevcut PDF'ler taranÄ±yor...")
    event_handler.scan_existing_pdfs(watch_directory)
    
    # KlasÃ¶rÃ¼ izlemeye baÅŸla
    observer = Observer()
    observer.schedule(event_handler, str(watch_directory), recursive=True)
    observer.start()
    
    logger.info(f"ğŸ“ Ä°zleniyor: {watch_directory}")
    logger.info("Yeni PDF eklendiÄŸinde otomatik olarak Ã¶ÄŸrenilecek...")
    logger.info("Durdurmak iÃ§in Ctrl+C")
    
    try:
        while True:
            time.sleep(10)
            # Her 10 saniyede bir durum mesajÄ±
            stats = event_handler.pc.Index(event_handler.index_name).describe_index_stats()
            logger.debug(f"Pinecone vektÃ¶r sayÄ±sÄ±: {stats.total_vector_count}")
    except KeyboardInterrupt:
        observer.stop()
        logger.info("Ä°zleme durduruldu")
    
    observer.join()

if __name__ == "__main__":
    main()